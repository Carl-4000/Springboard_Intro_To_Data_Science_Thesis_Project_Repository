---
title: "Basketball End-Game Scenario Analysis"
author: "Carl Larson"
date: "5/22/2018"
runtime: shiny
output: html_document
---

##Outline

- Problem Statement
- Data Story
- Goal of Project
- Product Prototype
- Ideas For Further Research
- Conclusion & Recommendations
- Bibliography

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1 Problem statement: 


According to the NBA's estimate, the legal potential market size for basketball gambling could be close to $200 billion, but it's a vastly under-quantified arena [Bonesteel, 2018]. 

“We might just base our opinion on what we saw in the first half and what we expect in the second half.” - Jay Kornegay, executive vice president of race and sports at Superbook Westgate Las Vegas, on setting odds during a game. While football odds are easy at halftime, Kornegay explains, "The NBA schedule might have four games at 4:05 p.m., then you get four halftimes around the same time, and the 5 p.m. or 5:30 games, plus college basketball games. It can really complicate things. You don’t get to discuss it, you just have to get numbers up." [Everson, 2015]

Between gigantic gambling markets and record-setting player salaries, it's becoming a very valuable question to ask, how late is *too late* for a comeback in basketball?

The risk of star players being injured (or even fatigued) is a big one, and teams are looking to minimize that risk, while still maximizing the likelihood of a victory. 

Vegas oddsmakers are hard-pressed to quantify the mathematical end-game scenarios of basketball the way those same scenarios have been mapped out in chess and poker, where we often see dynamic odds for each player winning a hand.

Further, it's interesting to fans to note whether a game is still competitive or mathematically out of reach. Fans' time is valuable too, and people would be interested to know if they can turn off a game and move on with their lives, or if they need to keep watching. 

#This is an optization problem, and to solve it we break it down into a binary classification - is it garbage time or not? 

##Data Story: Why this matters

Derrick Rose grew up in Chicago, and after being drafted with the number 1 pick in the 2008 NBA draft by the Chicago Bulls, he instantly became the hometown favorite. It's rare in sports that players represent their hometown team, and with a fresh $190 million endorsement deal with Adidas, Rose was on a track of stardom parallel to the great Michael Jordan.

Rose went on to win Rookie-of-the-Year in 2009, and then the league Most Valuable Player (M.V.P.) award in 2011. 

On opening day of the playoffs in 2012, Derrick Rose's Chicago Bulls were the number-1 ranked team in the Eastern Conference, ahead of the defending champion Miami Heat led by Lebron James. 

Hosting the 8th seed Philadelphia 76ers in game 1, Rose and the Bulls found themselves up by 83-66 with over 10 minutes remaining to play. 

The Bulls' coach chose to keep (at that point, the somewhat injury-prone) Rose in the game all through the 4th quarter, until they were in the final 80 seconds, with the score at 87-99, when Rose landed wrong on a jump and hit the floor writhing in pain, as Rose's teammates had to foul to stop play 13 seconds later.

The Bulls would go on to win game 1 with Rose carried off the court, but the *76ers* would then go on to make a historically rare comeback and win that series; an 8-seed beating a 1 seed has happened only about a half dozen times in NBA history. 

Rose's torn ACL kept him out for the entire following season. The Bulls team began to falter as key player contracts expired, and the Bulls have not since regained that level of greatness, having slumped to become one of the worst-performing franchises in the NBA today [Friedell, 2017].

The day of his injury, Rose was in the 2nd year of a jaw-dropping $92 million 5-year contract with the Bulls. 

By the 2018 off-season, Rose has just finished a $419k 1-year contract with the Minnesota Timberwolves. Needless to say, these avoidable injuries are bad for players, as well as teams, and fans (especially the Bulls').

Coaches are juggling a huge amount of dynamic information in the basketball end-game, and it's easy for emotion to get in the way of good coaching decisions. But if the "Moneyball" mindset has taught us anything, it's that actual data insights can improve executive level sports decision-making that previously relied on intuition or experience (possibly leaving new coaches out to dry; as the Bulls head coach was in his first year as a head coach, having been one of the league's most successful assistant coaches). 

This project aims to mitigate needless injury to relatively high-value basketball players, representing potentially dozens or hundreds of millions of dollars in the air per year (especially when considering the amount wagered on basketball; March Madness and the NBA total into the hundreds of billions of dollars per year).

Indeed, NBA players have become the highest-paid professional athletes in sports today[Badenhausen, 2016], and there has been scant attention paid to end-game NBA Garbage Time scenarios, to the extent that simple Gameflow Tables are hard to find. 

#Definition: Gameflow Table: 

Basically the expanded score, to include the time variable at each moment a team scored.

"In contrast to the usual score by quarters (or by half in college play), the graph easily allows complete identification of items of interest, such as largest leads, lead changes, come-from-behind activity, and percentage of game time for which a particular team of interest held a lead." [Westfall, 1990].

So rather than just reporting score at the end of each quarter like a basic box score, the Gameflow Chart shows the score in real time though the game as teams make their runs and go on slumps. It tells a detailed story of the game. 

Given the Minutes Remaining, and the score, it's possible to optimize declaring garbage time as early as possible to *mitigate injury risk*, while still *minimizing the risk of being wrong* and essentially giving your team an extra "Loss" by predicting garbage time wrong and too early. Mitigating injury risk means finding when it is safe to take star players out of the game to prevent injuries or fatigue.

#Definition: Garbage Time

NBA Hall-of-Fame announcer, Chick Hearn, is famous for coining the phrase, "Garbage Time" [Carlson, 2002], which means the time at the end of a basketball game when the score difference is too large for the game to be competitive. Having a stronger definition for Garbage Time is key for basketball statistics. 

Once Garbage Time is declared, it can't be taken back. 

Below we see a glimpse of NBA salaries broken down by age, [via NBA.com by way of Reddit](https://www.reddit.com/r/dataisbeautiful/comments/7mgh1l/age_and_salary_scatter_plot_for_nba_players/). This view is helpful, because it spreads out the data over a time-related x-axis so we can really get into the details, while still maintaining a scatter-plot format, which provides an ideally high density of information. 

![Like all salary data this is skewed-right or skewed-out, away from the graph-origin](https://i.redditmedia.com/lDaev_lEPjjymhoCuNxzyyIZ-mymKXhC84L-1YqA0rU.png?w=1024&s=aac78eeed479c27a8f8643136624e80e)

Basketball being a timed sport, the teams can only score at a certain rate, thus there is an envelope where the game goes from being classified as competitive, to out-of-reach for the trailing team simply due to the speed of time, and the speed of basketball. 

Aside from any considerations about skill or player chemistry, this analysis takes a scoreboard-only approach in simply communicating that mathematical envelope where *time itself* becomes the impossible obstacle.

Hundreds of millions of dollars of player time is risked every game like chips on the poker table- an ante for each moment of time players are left in the game. Not only are players sometimes injured in garbage time, but there are myriad situations in basketball (such as where a team is playing two games in a row and opponents are not) where it's vital to remove star players from the game as early as possible, to preserve their energy. It's not just about avoiding the big ACL-tear of the MVP, it's also about minimizing the soreness and tired legs that can affect the next game. Thus, this is something a very large swath of coaches and users are going to need every game. 

Minimizing injuries to star players, like then-reigning MVP Derrick Rose in 2012, is key, as is keeping the legs of your best jump shooter from getting tired or fatigued needlessly; thus this project set out to better quantify the end-game of basketball, so we can better measure when to keep star players in the game, thus minimizing those risks and having to pay those antes. 


```{r}
#To run any of the functions in this R Markdown document, it's necessary to run this snippet first to establish the relevant libraries and global variables

require(shiny)
require(data.table)
require(dplyr)
require(ggplot2)

#Enables console user-data input.

#minleft <- readline(prompt="Input minutes remaining: ")
#minleft <- as.numeric(minleft)
#secleft <- readline(prompt="Input seconds remaining: ")
#secleft <- as.numeric(secleft)
#scA <- readline(prompt="Enter the score of away team / team A:")
#scA <- as.numeric(scA)
#scB <- readline(prompt="Enter the score of home team / team B:")
#scB <- as.numeric(scB)
#MinzLeft <- as.numeric(minleft) + as.numeric(secleft/60)
#scdiff <<- abs(scA - scB)

#Hard-coded global variables rather than console-coded
ConfThresh <<- 0.9950   
GameLength <<- 48   
MinzLeft <<- 4.5  
scdiff <<- 9   
  
```

##Explanation of Variables

Score difference and Minutes remaining, above, are variables only to frame the y-axis and can't be too wide or it can overwhelm a laptop-grade computer. Minutes-Left should be between 3 and 8, and Score Difference should be between 4 and 25. But ultimately, part of the fun is creating your own example numbers to see how it runs differently, and how the eventual contour map is affected by say, changing the framing score difference used. The contour map is written to display uncertainty as "water," and certainty as "high ground," so in a sense, keeping everything else constant and rendering contour maps for decreasing score differences is like watching a topography flood with water from space.

These variables have been set to workable defaults, and hashed-out are lines of code which would allow users to be able to enter this data from their console. Ultimately, this snippet was functionally obsoleted by the UI inputs of the Hooply app itself, but before Hooply was made, these were those inputs.

#"MinutesLeft" 

In this document, minutes left needs to be under 10 for below functions to work, otherwise it breaks the app and yields error messages after long pauses. To run large simulation batches over 500 games per scenario (roughly 150k games or 33 million rows), Minutes-Left needs to get even smaller (closer to 3 or 5). Minutes-Left actually serves as a placeholder, eventually framing the x-axis of the product graph, and it just can't be too large at this point because it raises the eventual computing burden too high. 

#"ConfThresh" 

Standing for confidence threshold, setting this variable to 80% is more conservative (less risky, i.e. pre-season); 99.9% is more aggressive or risky (i.e. crucial playoff game).

#GameLength 

Simply regular game length in minutes; it should be functionally irrelevant whether 40 or 48 or whatever since this is a purely end-game analysis, just needing consistency throughout.

Above is a preview of the application input variables of the user's game. This input is saved globally via the "<<-" operator, and later once we start looping simulations, we'll be adding a score buffer of "scdiff" to attain the desired relevancy for the user scenario. 

##Project Goal: Inputs and Outputs

The goal of this project from the inception was about measuring "Garbage Time" in basketball, as coined by NBA hall of fame announcer Chick Hearn. 

The goal of this document itself is to also show how this come together, as well as provide the key R code snippets that went into building this application. 

To do this, first it was needed to list out the necessary inputs and outputs: 

Inputs: 

- time remaining (minutes:seconds)
- team A score
- team B score
- advanced setting: confidence interval default at 99.5%

With these values set as inputs, organizing the outputs was key for this project. 

Outputs: 

- % time a team came back 
- graph of nearby values (holding score diff constant) each with y-percentage value for say 30 final game-flow actions or more if the user requests some time very early.
- User-indicator that displays whether the % likelihood is above or below the confidence threshold (i.e. 99.5%). 


#Data Collection vs Generation

After experiencing frustration seeking  large quantities of game-flow tables that can be analyzed in R using data mining, the decision was made for this project to use simulated basketball data based on a model rather than mined data, which creates the need to run model validation checks. 

##The Garbage Time App

To accomplish this goal, this project's ultimate product is the "Hooply" web app based in Shiny R, which allows users to key in their specific time-remaining, and score for each team. The app delivers a responsive recommendation on whether the game is in garbage time, and also shows a contour chart detailing the other "nearby" scenarios on the x-y map of time and score-difference combinations.

#How This App Was Made

This app was made to create a data science recommender product for basketball coaches, so they could know whether the game was mathematically over or not, given the average rates at which teams score. There were a few failed strategies, but those are given mention towards the end of this document. 

For now, this outline will show the functions that were built to be able to create this app. This broke down into the following steps: 

- The Gameflow-table maker function
- The Gameflow-reducer reader function
- The chart-matrix creator function 

For future work it would be interesting to see whether there could be a way to cleverly accomplish the above in two (or even one incredibly complex) function, but for now, it would suffice for the project goal to focus on the above three. 

##Gameflow Table Generator

Because game-flow charts are difficult to come by in large quantities, this project features a Gameflow Table generator function in r, which generates a game-flow chart from scratch, using R's rnorm, sample, runif, which, and cumsum functions. 

This first function takes 3 user-defined variables, probA, probB, and n. These variables come with defaults that end up producing sensible data, but it's left open for users to tinker with. The default for "n" creates a normal distribution that allows games to have a sensible length.

First the need was to quantify the rates at which teams score, and to this end we have the "GameflowMaker4()" R-function which was built using normal distributions for team scoring probabilities through the game. So for each individual game flow table, there was a unique set of values of probability of scoring for each team. This ensures that each game witnessed a truly unique dynamic controlling scoring probabilities, as well as the fact that the nature of the normal distribution binds the range of potential outcomes to keep values realistic.

##Explaining GameflowMaker4

This function creates a basketball game-flow chart from random variables, including rnorm, sample, and runif. 

5 optional arguments are included, including GL for "Game Length," in minutes; this allows users to change the length of the game to match their level of basketball. Pro-basketball games are 48 minutes, and college games are 40 minutes, for example. This allows coaches to better-tailor this function to their specific length of game, but is mainly cosmetic since this is an end-game analysis.

The function uses "abs()", because if the user changes probA or probB such that any of the below values become negative, it will break the code. Please note probA and probB are pre-assigned to be values that will virtually never yield zero or negative results, based on the means and standards of deviation. 

##Minimum delta t

The issue of having a minimum increment of time between scores was key. So listA serves this purpose, by storing a sequence of the minimum distance between each action. Then the later rnorm() function is added to 'listA,' randomizing the output, and preserving the original distances (t) between each value.

Conceptually speaking, this prevents having too many scores too quickly due to random chance, which would eventually allow a small percentage of teams to come back in an unrealistically aggressive way. This fact of basketball physics is represented as a minimum delta t, a function argument suggestively pre-set to t = 0.2, representing the minimum amount of time that has to elapse before the next team scores.

## What "dt" values make sense? 

First we note that "t" is engineered to become ML, or the "Minutes-Left" column, representing the game clock counting down. ML is encoded to be minutes remaining, so it is not in seconds, but minutes. It wouldn't make sense to have t be over a few tenths of a second, which translates to minutes (.3 /60 = 0.005) such that it doesn't make sense to have t values over 0.01, because the next team can score faster than in 0.01 minutes, or 0.6 seconds. 

For example, it's often the case late in a game that as soon as a player of a team with the lead catches an inbounds- pass to start the clock, the other team will immediately foul, to stop the play and preserve time on the clock. Often, referees will quickly call the foul to avoid players hurting each other by fouling too hard, and this results in often for example only three tenths, or five tenths of a seconds will have come off the clock before the next team is scoring. Teams do that literally to stop the game-clock in hopes that the player misses free throws and they can get the rebound. It's a desperate strategy, akin to how in hockey late in a game, coaches will sub out a goalie and sub-in an extra attacker, or how in American-rules football, teams will try an onside kick after scoring a touchdown. Not that we're covering hockey or football in this project, but as timed sports, it's certainly within the realm of mathematical possibility for future projects. This type of analysis doesn't apply to non-timed sports, like gymnastics, baseball, or tennis.xxx

##nrorm() Usage

Supported by the 1976 Mertens and Zamir paper on game theory between two competitors in zero sum environments, normal distributions were used as the foundation of the GameflowMaker4 simulator. This makes intuitive sense according to the Central Limit Theorem [Mertens and Zamir, 1976].

With R's rnorm() function, the first argument is the number of variables to yield, the second is the average, or mean, and the third argument is the standard of deviation. A relevant normal curve is created and 'n' variables are returned from that curve. It is a very useful function in the R language when dealing with random variables that correspond to comparisons between people, such as IQ or SAT tests, personality tests, or in this case basketball ability of a team. rnorm() ensures we will have unique teams, with unique skill levels, playing each game. This guards against algorithmic bias, because we create different values of probA and probB for each chart.

```{r}
#First we construct a crude but effective basketball 
#gameflow table generator. It returns a gameflow table,
#which can easily be looped or used to make a gameflow #chart. 
#One ggplot-chart is provided below the function in the snippet
#as a sample, and another sample grapher using base-R is hashed out. 

GameflowMaker4 <- function(
  probA = c(abs(rnorm(1, 0.65, 0.005)), abs(rnorm(1, 0.09, 0.002)), abs(rnorm(1, 0.17, 0.005)), abs(rnorm(1, 0.09, 0.005))), 
  probB = c(abs(rnorm(1, 0.64, 0.005)), abs(rnorm(1, 0.10, 0.003)), abs(rnorm(1, 0.17, 0.005)), abs(rnorm(1, 0.09, 0.005))), 
  n = as.integer(abs(rnorm(1, 218, 22))),   #ad hoc game-length index variable
  dt = 0.005,   #min time between scores to allow for physics
  GL = 48){   #game length of 48 minutes per NBA rules
  sta = sample(0:3, n, replace=TRUE, prob=probA)
  stb = sample(0:3, n, replace=TRUE, prob=probB)
  w = which(sta > 0 & stb > 0)
  sta[w] = 0
  stb[w] = 0
  v = which(sta == 0 & stb == 0)
  sta <- sta[-c(v)] 
  stb <- stb[-c(v)]
  m <- length(sta)
  listA <- as.vector(seq(from=0, to=((m * dt)-dt), by=dt))
  listB <- as.vector(sort(runif(m, 0, c(GL-listA[m]))))
  MinLeft <- listA + listB
  MinLeft <- rev(MinLeft)
  scoreA <- cumsum(sta)
  scoreB <- cumsum(stb)
  df = data.table(MinLeft, scoreA, scoreB)
  if(df$scoreA[nrow(df)] == df$scoreB[nrow(df)])
  df = df[-c(nrow(df))]
  return(df)
}
df <- GameflowMaker4()
#plot(df$MinLeft, (df$scoreA - df$scoreB), type = "b", main = "Sample Gameflow Chart",
     #xlab = "Minutes Left", ylab = "Away team score minus Home team score")
ggplot(df, aes(x=df$MinLeft, y=(df$scoreA-df$scoreB))) +
  geom_line() +
  labs(y="Away team score minus Home team score", x="Minutes Left") +
  ggtitle("Sample Gameflow Chart") +
  scale_x_reverse()

```

This function, Gameflowmaker4() essentially was the heart of the app; with each "heartbeat" a simulation was performed, where at time "x" and score difference "y", representing the current time and score difference respectively, some "m" number of games are simulated, say a hundred or a thousand, or even 9,000 as was the case for this project. The default "m" value below is set to 50 to keep computation loads small by default, but bumping "m" up by one or two zeros does not hang the program, it just takes an extra 5 or 10 minutes to finish rendering.

Running the above code snippet shows how a game-flow chart works for this simulation, and is necessary to establish this function or later usage in this R Markdown document. The inputs are various possible user-defined arguments that control normal distributions that combine to form random variables. The output of this function is a single matrix with 3 columns and with a row for each unique made basket.

As you can see above, this creates a typical game-flow chart, with a few mentor-stipulations for this project, which were: 

- to delete from the matrix all rows where a team was not scoring
- to delete all rows where both teams were scoring (which was occurring based on how I built it)
- to add a minimum amount of time (dt as discussed above) that had to elapse to allow for the ball to get up the floor physically 

Next we need to loop with this function to create a new function that simulates 300 relevant games for the user. 

##Exploratory Visualizations

The first graph used to explore this data happened to be a style of presentation already published by Franklin Kenter in a paper [ESPN would later call "hacking basketball."](http://www.espn.com/nba/story/_/id/12954130/let-hacking-begin) 

![#Kenter's Choice of Axes on Figure 1 of his paper (below)](/Users/EagleFace/Documents/Hooply_app/Kenter_graph.png)

![#Goel's Contour Plot (below)](/Users/EagleFace/Documents/Hooply_app/Goel_Contour_Plot.png)

![#Goel's Formula vs Michael Lewis's Reported Rule of Thumb (below)](/Users/EagleFace/Documents/Hooply_app/Goel_Formula_Plot.png)

Above we have a brief review of how this data can be sliced. We won't be making any predictions as specific as Kenter, whose paper recommends actual player actions based on score-time scenarios[Kenter, 2015]. 

With Goel's data, he breaks down "Moneyball" author Michael Lewis's approach to this problem, in offering a rule of thumb that if a team is up by as many minutes as there are left, there is an 80% chance of winning for the leading team (Lewis, 2004). Goel finds that rule breaks down under 6 minutes, and offers a different rule of 2*sqrt('Minutes Left') as the conversion to score, still holding 80% constant. Goel's analysis doesn't depart from this 80% metric, and that creates one area this project explores. Essentially, 80% is a very low metric, one in five, and coaches will need more certainty. 

Further, these graphs above are depicting discrete data (score difference) as the graph output (or contour lines). Discrete data makes a much better Y-axis, so this paper will set out to eventually do that and leave the continuous variable, percent win chance, as the graph output, which as a continuous variable, can be much more naturally color coded and mapped out than step-wise score differences. 

##Looping Batches of Games

Below, "m" is essentially a "focus" meter; the eventual graph of probabilities might be out of focus with a lower "m" value like 200 (which requires less computing power) and would be in, essentially, greater focus with an "m" value more like 1000, meaning, 1000 game-flow charts were used per game action index (the game action index literally being only a made basket by either team, but also acting as an index counting down the final relevant "basketball-actions" allowing other meaningful variables to be in the same row, such as time, which eventually creates the x-axis, and percent chance, or the y-axis). 

At this juncture, there was a need for the function "algo2app," which was tailored to output key data such as who was leading by what at about which time in the game, and then who won that game? This was required to know the leading team's win percentage out of a certain scenario. With m = 900 simulations per scenario you can start to see some very smooth contour lines, but the default data was turned down to 90 to lessen computing time, in case someone wants to casually run the below snippet without changing any figures, the code should run fairly quickly. 

```{r}

#"h" here (for "hoop" literally since made-baskets create this index) controls 
#the index of the function, defaulting to 30 or the 30th-to-last 
#made basket by either team, allowing for easy looping later. 
algo2app <- function(h=30, p=scdiff){
  m=90
  mec <- matrix(data = 0, nrow = 1, ncol = 5)
  for(i in 1:m){
  df <- GameflowMaker4()
  q <- (nrow(df) - h)
  mec[1,4] <-  mec[1,4] + as.numeric(df$MinLeft[q])
  mec[1,5] <- mec[1,5] + as.numeric(nrow(df))
  if(((df$scoreA[q]-p) > df$scoreB[q]) && (df$scoreA[nrow(df)] > df$scoreB[nrow(df)])) {
  mec[1,1] <- mec[1,1] + 1
  next} #team A led at q and eventually won
  
  else if(((df$scoreB[q]-p) > df$scoreA[q]) && (df$scoreB[nrow(df)] > df$scoreA[nrow(df)])){
  mec[1,1] <- mec[1,1] + 1
  next} #team B led at q and eventually won
  
  else if(((df$scoreA[q]-p) > df$scoreB[q]) && (df$scoreB[nrow(df)] > (df$scoreA[nrow(df)]))){
  mec[1,2] <- mec[1,2] + 1 
  next} #team A led at q but team B came back and won
  
  else if(((df$scoreB[q]-p) > df$scoreA[q]) && (df$scoreA[nrow(df)] > df$scoreB[nrow(df)])){
  mec[1,2] <- mec[1,2] + 1
  next} #team B led at q but team A came back and won

    else{mec[1,3] <- mec[1,3] + 1 #functional "q-ties" irrelevant
      next}
if(i==n)
  break}
  mec2 <- matrix(data=0, nrow = 1, ncol = 4)
  mec2[1,2] <- (mec[1,1]/(mec[1,1]+mec[1,2]))
  mec2[1,1] <- h
  mec2[1,3] <- (mec[1,4] / m)
  mec2[1,4] <- (mec[1,5] / m)
  colnames(mec2) <- c("Game_Action_index", "Leader_win_chance:", "Avg_sample_MinsLeft", "avg_Game_Actions")
  return(mec2)
}
#Below is a couple of sample runs of this function as an example.
algo2app()
algo2app(h=10)

```

The above function "algo2app" was one of the first functions used in this project to usefully reduce the dimensions of potentially thousands or millions of game-flow chart-style basketball game simulations. 

Below a for-loop is explored to print out "algo2app" over multiple basketball end-game scenarios. Ultimately this function was obsoleted by other more robust functions, but this was still required to complete a "slice" or a Kenter / Goel style graph. 

```{r}
#This runs for the argument "k" game-action index down to 1 creating a dataframe
#of row number k and column number 4. Basically expands the above function into a loop, 
#nothing too fancy but very needed.
GFmapper <- function(k=35){
  matx <- matrix(data = 0, nrow = k, ncol = 4)
  h <- k
  for(i in 1:k){
    h <- h
    dfJ <- as.matrix(algo2app(h=h))
    matx[i,1] <- dfJ[1,1]
    matx[i,2] <- dfJ[1,2]
    matx[i,3] <- dfJ[1,3]
    matx[i,4] <- dfJ[1,4]
    h <- h - 1
    if(h == 0){
      break
      }else{
        i <- i+ 1
        next}}
  return(matx)
}
```

##Depicting Multiple Basketball Games On One Plot

Given a bit of feature engineering, there a number of different angles to analyze game-flow charts. Large amounts of game-flow charts are multi-dimensional; the three directly relevant variables are: 

1. a score difference between two teams
2. time remaining
3. a percent likelihood of the leading team winning the game (or one minus the possibility of a comeback)

Some approaches have had success plotting this with time on the x-axis, win chance on the y-axis and score difference labeled as a makeshift z-axis. 

This is graphed below, essentially holding score difference as one constant line, and simulating "m" number of games from the algo2app function. 

Graphically this is one slice, or a horizontal cross-section, of the eventual contour map we will plot.


```{r}
#Holding score constant at the global variable "scdiff" we can see how probability of a comeback decreases over time
ContMapSlicer <- function(k = 35){

df <- as.data.frame(GFmapper(k=k))
colnames(df) <- c("Game_Action_index", "Leader_win_chance:", "Avg_sample_MinsLeft", "avg_Game_Actions")
#plot(df[1:k,3], df[1:k,2], type="b", main="Leading Team Win Chance",
#     xlab = "Minutes Left", ylab="Percent")
ggplot(df, aes(x=df[1:k,3], y=df[1:k,2])) +
  geom_point() +
  geom_smooth(span = 0.2) +
  geom_hline(yintercept = ConfThresh, linetype="dotted", color="red") +
  geom_vline(xintercept = MinzLeft, color="green") +
  labs(y="Percent", x="Minutes Left") +
  ggtitle("Leading Team Win Chance")
}
ContMapSlicer()
```

The green line represents the user's actual reported minutes left, and the red dotted line is the 99.5% cutoff for sensible confidence that "garbage time" has begun. Essentially when the blue line goes above the red line, and thereafter for the rest of the game, garbage time is in effect, irrevocably (it makes zero sense to go back on garbage time; essentially the algorithm just needs to either be right or wrong or not declare garbage time, declaring garbage time is a one-way street).

This graph essentially shows for the user's current score difference (which is held constant for mathematical purposes), the leading team's chance of winning as the clock winds down, given that score difference. With the range of potential end-game score differences, it would be possible to compute yet one final loop for 1:(1.2 * scdiff); essentially putting many of the above single lines side-by-side, to create a topographical map that essentially provides an excellent window into the data itself, in this case the Gameflow4() basketball game simulator.

This type of analysis could even be used to easily and insightfully measure different basketball simulators, as well as look at large amounts basketball data, such as ever NBA game ever played, which number over 400,000. 

#Using Linear Regression

The reason we would want to multiply scdiff by a "1.2" or something there (to be determined partially by regression, and just for the goal of roughly centering the user's position on the topo-map) is that we need a metric to convert the minutes left to a game-action-index and back. Essentially, this is a function of many inputs via the Gameflowmaker4() function, and we are measuring all these inputs with one output which would be essentially the average rate at which baskets are scored, or, roughly, "Minutes-Left" per "GameActionIndex." This will allow us to create a relevant frame of y-values for the "scdiff" axis we will show the user in the final graph along with the program's recommendation. This will also allow us to convert the user's "Minutes Left" reported global variable and essentially convert that to the most appropriate "k" value to enter into the GFmapper function, which logically cannot just take a clean "Minutes Left" cross-section because it's impossible to just force a game-action (a basket being scored) for no other reason without affecting the score and the potential outcome, and biasing the results. It would also be unconscionably inefficient to throw out 99.9% of results because they didn't include a basket being scored at *precisely* 5.00 minutes remaining, especially because scores in basketball change so quickly, it's over-thinking things to go that specific when looking at minutes remaining. 

During basketball games, score differences are never static for long. The index is under the control of no one single player or team. So rather than seeing the graph above and potentially getting confused with multiple graphs, really the one graph the user will need to see is the topographical map of what would happen given different potential score changes, and see them all side by side, because the whole y-axis in the graph above can be flattened into a single color bar for each index value or "k" -value above, where k=35.

So rather than just saying "to wit, k = 35" we need to be able to treat "k" as the entry point for the logic of MinzLeft's conversion into an index. This essentially yields "x_framer" below, which just is to help frame the topographical map for the user's position.

We can actually re-use this regression formula to find the extreme case of the rate of a comeback by one team. If one team is scoring all the baskets, how fast can they come back? Well that's determined by the regression above, which we can use below to say, assume one team is scoring all the baskets, what is their pace of coming back? This is just a rough estimate to help relevantly frame the y-axis based on the most extreme example. 

Closeout chance is used below, rather than comeback chance, just because closeout chance corresponds to percent confidence intervals, and we can apply some according statistical significance, in that comebacks were occurring in our simulations only x-percent of the time in this situation. 

##Note On The Round Function

Ultimately, rounding up or down to an index from continuous time is the challenge. The loop index must be in integer format, but time comes from the user in continuous format. The issue is border cases where users are close to two different scenarios; whether to round those users down to a common index, or up. The choice was made to round up, which possibly introduced bias, because some edge case users may be seeing higher probabilities than they should, because in the rounding up, the machine thinks there is more time in the game than there actually is - never by more than 30 seconds or so, as will be shown by the linear regression below. 

Later in the app itself, the same value is then rounded down to the nearest integer using the floor() function, ideally to compensate slightly for the rounding bias above. 

##Editing algo2app()

It was good that algo2app() output 4 variables, which enabled the regression to create the map framing equation. However at this point we want more efficiency, and best would be roughly the same exact function, except outputting only one variable given the 2 inputs of h and p; given that one extra column is needed for the index, which itself needs to be an extra variable because it is used for looping. 

##Linear Regression From Minutes To Index

One issue with this project is that it has to quantize otherwise continuous time. Essentially this is like putting a cage in an open space to be able to constrain a variable; the coefficients on this linear regression are where the bars go. 

Essentially, this linear regression allows us to take "Minutes Remaining" formatted time data and reliably convert it to "Index" formatted time data. This allows the loop to roughly understand where the user is in the map.

The Y-vector below is actually just the values 1 through k, in this case K is set to 35.

To increase the number of simulations run in this regression, refer up to algo2app and increase m. 

```{r}
#Goal here is to run linear regression and find the slope (or calculus derivative) 
#of the two variables Avg_sample_MinsLeft and game_action_index. 
#This will allow us to later graph relevant ranges for score differences

k <- 35
df <- as.data.frame(GFmapper(k=k))
colnames(df) <- c("Game_Action_index", "Leader_win_chance:", "Avg_sample_MinsLeft", "avg_Game_Actions")

X <- df[1:k, 3] #Minutes left
Y <- df[1:k, 1] #output game action guess

LinReg <- lm(X ~ Y, data=df)

print(LinReg)
```

This shows us the line-of-best-fit that was drawn through the average minutes-remaining values at each of the 35 final baskets made. 

This function runs the algo2app dimension reducing function on the GameflowMaker4 simulator. Recall that in algo2app, m was the number of simulations to be run per quantized unit of time. This means "m" is essentially a meter for "focus," allowing us to increase the focus of our data on the "true" values, at the cost of increased computing time. 

Running this at a focus value of m = 200 yielded a slope of 0.4835, which is right on target for what would be expected; teams scored a basket on average every 0.4835 minutes (or 29.01 seconds), which is roughly 2 baskets a minute, or one basket for each team on average, or about 2 points per minute per team. 

We can also increase the focus and run an m-value of m = 1000: this yielded a slope of 0.4847, just a hair above the lower-focus value; also it had a y-intercept essentially representing the final basket of the game of roughly the same size as the slope. I will go with the higher-focus value of m=1000, meaning, one thousand game-flow simulations were ran for each game-action-index, of which there were 35 in total across the x-axis, and if you run this yourself you'll see it takes much longer to compute 35,000 game-flow charts than 7,000 (more than 5x as long). 

Game_Action_Index = round(MinzLeft * 0.4847 + 0.4733)

This allows us to convert it to the nearest game-flow action. We'd be able to multiply the total number of game actions by this equation and get the computable game-flow action the user meant by the amount of minutes remaining. Again, by using "round" here there has to be some bias introduced, one way or the other, to get the user's continuous time to a specific pre-computed game index. 

##Model Validation

So now that we have a model, we can do some basic empirical model validation. Dr. Sharad Goel of Stanford performed an analysis using data collected from 7,000 games over the past decade, to answer these same questions, in his write up, "How Close Is Close?"

"Basketball games can be reasonably well modeled as a series of independent one-minute rounds consisting of approximately one possession for each team. In each interval, the score differential between the teams changes by about two points, with each team roughly equally likely to win that round. In statistical terms, the intervals are approximately mean 0 with standard deviation 2. The central limit theorem then shows that with t minutes remaining, the score gap changes approximately according to a normal random variable..." 

Goel's reference to the central limit theorem supports the logic from the 1976 Mertens and Zamir paper detailing how zero-sum two-player game outcomes are subject to normal distributions. 

To better understand how these variables would be output by our model, we can look back to our previous regression run from the above code snippet. 

As long as both of these values are reasonably close to one half, that means that each team is getting a half minute to have a possession. This is considered by Goel to be a mark of a reasonable basketball simulation and serves as excellent model validation. 

By using a linear index, we can run a linear regression to estimate the relationship between scoring and time. Finding how often a basket is typically scored allows us to translate easily back and forth between "Front-facing" time with minutes-left, and then back-end facing time with the made-hoop index needed for looping. 

```{r}
#Designed to print out home win percentage running m-many game simulations
#to compute the home and away team win percentages
#m is set to 50 to represent a someway significant value, but still nothing that
#would overly tax computing power. That said, this type of simple simulation
#can run thousands or millions of times (if you have the patience!). 

HomeWinQApp <- function(m=50){
  
  mec <- matrix(data = 0, nrow = 1, ncol = 2)
    for(i in 1:m){
    df <- GameflowMaker4()
      if((df$scoreA[nrow(df)]) > df$scoreB[nrow(df)]){
        mec[1,1] <- mec[1,1] + 1   #away team won
        next} 
      else if(df$scoreA[nrow(df)] < df$scoreB[nrow(df)]){
        mec[1,2] <- mec[1,2] + 1     #home team won
      next 
      } else if(i==m){break}}
    mec2 <- matrix(data=0, nrow = 1, ncol = 2)
    mec2[1,1] <- (mec[1,1]/m) #away team record
    mec2[1,2] <- (mec[1,2]/m) #home team record
    colnames(mec2) <- c("Away Team Record", "Home Team Record")
    return(mec2)
}
#A sample run is included
HomeWinQApp()

```

##Model Validation

Sharad Goel's write-up on garbage time in basketball provides a great foundation for how to judge a statistically rigorous basketball game simulator [Goel, 2012]. Goel's write-up, along with a few other metrics, will be used as model validation in this section. 

Below, we can start to open up GameflowMaker4 with various functions to essentially treat it like its own world, or type of coding, that can be queried scientifically and potentially invalidated. 

With the custom function "finalScoreAverager," we can increase the number of Gameflow Tables that go into the figure. The "n" variable below controls the number of games to simulate to use to focus in on a true value for average away and home scores of teams. 



```{r}
#Creating a base function to use in a loop
finalScoreAverager2 <- function(n){
  df<-GameflowMaker4()
  AvgScore <- c(df$scoreA[nrow(df)], df$scoreB[nrow(df)])
  
  return(AvgScore)
}
finalScoreAverager2()
```

Above we have a function that averages the scores of the home and away team. Below, this function is then looped many times to try to find a more accurate figure. 

```{r}
#Game flow table score mass averager; works for m-values up past 10,000

GFTsma2 <- function(m=50){
  totalA <- 0
  totalB <- 0
  for(i in 1:m){
  tuple <- finalScoreAverager2()
  totalA <- totalA + tuple[1]
  totalB <- totalB + tuple[2]
  }
  totalA <- (totalA / m)
  totalB <- (totalB / m)
  return(c(totalA, totalB))
  }
GFTsma2(m=1000)

```

#Context

Depending on the year, average scores for NBA games have fluctuated between 91 in 1998 and 118 in 1962. Values in this range are acceptable as realistic. 

With this, we can see that the scores correspond roughly to average NBA scores, or in other words, these numbers aren't wildly unbelievable for NBA scores.

"As explained by Moneyball author Michael Lewis, 'one statistical rule of thumb in basketball is that a team leading by more points than there are minutes left near the end of the game has an 80% chance of winning. If your team is down by more than 6 points halfway through the final quarter, and you’re anxious to beat the traffic, you can leave knowing that there is slightly less than a 20 percent chance you’ll miss a victory.' While that’s a compelling heuristic, an analysis of over 7,000 games over the last six seasons reveals that it only holds for a few short minutes, losing accuracy even by the six-minute mark in Lewis’s example. A better rule of thumb is that a team has an 80% chance of winning if they lead by twice the square root of minutes left. Going into the final quarter (12 minutes) of the game, for example, that 80% threshold is achieved at 2√12≈7 points. By comparison, the standard heuristic suggests a substantially larger gap of 13 points is required to achieve an 80% chance of success...

"Basketball games can be reasonably well modeled as a series of independent one-minute rounds consisting of approximately one possession for each team. In each interval, the score differential between the teams changes by about two points, with each team roughly equally likely to win that round."

Interestingly in his write-up, Dr. Goel outlines the map of probabilities of end-game scenarios (such as 80%) without going as far as to present a graph of score-time scenarios with win probability on the z-axis. Holding win chance percentage as constant, Goel outputs different discrete score differences. 

Further, it's interesting to note that the Hooply data provided below conforms to Goel's analysis outlined here.

We can now revisit the ContMapSlicer() function to see what our model shows with regards to Goel's main 3-dimensional landmark, which was: 

- 12 minutes
- 7 points 
- 80% confidence

This just requires a call to ContMapSlicer() having set scdiff to 7, we can find 12 minutes on the graph and see what percent it suggests. 

```{r}
#The green line represent's the user's time remaining
MinzLeft <<- 12
scdiff <<- 7
ContMapSlicer()

```

This is interesting in that it shows the model is has higher confidence than Goel's data given his landmark of 80% confidence at 7 points lead with 12 minutes. The Hooply model reports confidence of roughly 94% in that particular scenario. 

Ultimately this could be compensated by the logic of the application itself, given that the threshold for a recommendation is still within our control, that allows users to tune their compensation for any perceived bias in the Hooply data. 

In a sense, this model validation shows roughly a 14% bias (80% vs 94%). Will users be able to perceive a 14% bias? It's possible, over the long term. Ultimately, this does require more empirical data and model validation, but it's an interesting window to compare to Dr. Goel's data. 

```{r}
#Resetting some global variables
MinzLeft <<- 4.5
scdiff <<- 8
```

##Creating The Contour Map

The contour map itself is a 3 dimensional graph where the "topology" is the percent likelihood of a team winning. Since percent likelihood can be so easily reduced to a single dimension of data, it makes a perfectly natural fit for the z-axis of the contour map shown below, where the x and y axis are framed to depict score-time scenarios that are generally relevant to the basketball end-game.

To do this, first a dimension-reducing intermediary function was created called "algo3app" that applied the Gameflowmaker4() function to a specific value of time and score, and this function is repeated below. Note, "algo2app" was built to the same purpose but output slightly different values than "algo3app."

For "algo3app" to work, however, there need to be "Minutes Left" and "Score" data for each team. So to make that work what's required are some default input values for the function - feel free to change those values and hit "Play" and see how they affect the output. 

#Dimension Reduction

To reduce the dimension of a large amount of game-flow charts, the strategy below is to average the outcome given specific inputs for score and minutes remaining. Leaving those 2 variables as function arguments allows the function to be later used in a loop where those two values become the x and y axis. 

Basically, this reduces one game-flow chart to a comparison of two scoreboard snapshots; one at roughly the user's location in each game, and another snapshot at the end of each game to determine who actually won. In comparing those 2 snapshots logically, the program counts up how many games fall into each category, and then divides out by the total to arrive at a total average, which is reported to the user for their closest time-score combination.

Ultimately, this function is tailored to create the kind of output that will be required by the z-matrix of our contour plot:

```{r}
#"h" for hoop and "p" for point difference.
algo3app <- function(h=MinzLeft, p=scdiff, m=50){
  
  mec <- matrix(data = 0, nrow = 1, ncol = 4)
  for(i in 1:m){
  df <- GameflowMaker4()
  q <- (nrow(df) - round(h / 0.4847 + 0.4733))
  mec[1,4] <-  mec[1,4] + as.numeric(df$MinLeft[q])
  
  if(((df$scoreA[q]-p) > df$scoreB[q]) && (df$scoreA[nrow(df)] > df$scoreB[nrow(df)])) {
  mec[1,1] <- mec[1,1] + 1
  next} #team A led at q and eventually won
  
  else if(((df$scoreB[q]-p) > df$scoreA[q]) && (df$scoreB[nrow(df)] > df$scoreA[nrow(df)])){
  mec[1,1] <- mec[1,1] + 1
  next} #team B led at q and eventually won
  
  else if(((df$scoreA[q]-p) > df$scoreB[q]) && (df$scoreB[nrow(df)] > (df$scoreA[nrow(df)]))){
  mec[1,2] <- mec[1,2] + 1 
  next} #team A led at q but team B came back and won
  
  else if(((df$scoreB[q]-p) > df$scoreA[q]) && (df$scoreA[nrow(df)] > df$scoreB[nrow(df)])){
  mec[1,2] <- mec[1,2] + 1
  next} #team B led at q but team A came back and won

    else{mec[1,3] <- mec[1,3] + 1 #functional "q-ties" irrelevant
      next}
if(i==n)
  break}
  mec2 <- matrix(data=0, nrow = 1, ncol = 3)
  mec2[1,2] <- (mec[1,1]/(mec[1,1]+mec[1,2]))
  mec2[1,1] <- q
  mec2[1,3] <- (mec[1,4] / m)
  colnames(mec2) <- c("Game_Action_index", "Leader_win_chance:", "Avg_sample_MinsLeft")
  return(mec2)
}
#Below are 2 sample runs of this function as an example.
algo3app()
algo3app(h=5, p=12)
```

Rather than taking an index and a score difference like algo2app, algo3app here actually takes the users' raw time remaining and automatically converts it, using the earlier linear regression logic. 

This allows for easier looping in the below function. This function below was used to create the contour map used in the final app. 

With this function in place, we can now easily call a function "gftopo4," which maps a contour map of game-flow outcomes as sliced by the function "algo3app," which itself is a dimension reduction of our original GameflowMaker4 core-simulation. 

The graph below depicts the quotient of the number of times the leading team at the x,y scenario ended up winning the game. 

Basically, given say, 10 minutes remaining, you can look up and see how much of a lead a team would need to start feeling confident about the outcome of the game. With 10 minutes remaining, a team would need to have over a 20 point lead to feel like it was garbage time. 

```{r}

gftopo4 <- function(m=50){
  MinzLeft = MinzLeft
  scdiff = scdiff
  x_framer <- round(1.67 * (MinzLeft / 0.4847 + 0.4733))
  y_max <- (scdiff *2)
  y_min <- round(scdiff / 4)
  i <- 1 #x index set before looping
  dfz <- matrix(data = 0.50, nrow = x_framer, ncol = (y_max - y_min)) #z matrix
  xax <- matrix(data = 0, nrow = x_framer, ncol = 1) #x axis spacing
  yax <- as.vector(seq(from = y_min, to = y_max)) #y axis
  for(i in 1:x_framer){
    xvr <- 0  #average variable that will sum and divide by n for columns
    j <- 1 #y index set before looping
    for(j in 1:(y_max-y_min)){
      df2 <- matrix(data=0, nrow=1, ncol=3)
      df2 <- algo3app(h=i, p=j, m=m)
      xvr <- xvr + df2[1,3]
      dfz[i, j] <- df2[1,2]
      if(j < (y_max+1)){
        j <- j + 1
      } else {
          break
        }
    }
    xax[i] <- xvr / (y_max + 1 - y_min)
    if(i < x_framer){
      i <- i + 1
    } else {
      break}
  }
  contour(x = xax, y = seq(from = y_min, to = y_max, length.out = ncol(dfz)), z = dfz, levels = seq(from = 0.9, to = 1, by = 0.025), col = topo.colors(5),
  xlab = "Minutes Remaining", ylab = "Point Lead")
  title("Simulated Win % of Leading Team", font = 4)
  legend(x=2,y=16,legend=c("90%", "92.5%", "95%", "97.5%", "100%"), fill = topo.colors(5), col = topo.colors(5))
  
#  write.csv(dfz, "/dfz.csv")
#  write.csv(xax, "/xax.csv")
#  write.csv(yax, "/yax.csv")
}
gftopo4(m=90)
##Note - be careful running this code as without the 3 pound-cancels above, it automatically writes files as well as creating an example contour plot. You would need to input your own filepath or let it write itself to the directory. 

```

The graph above here is more fragmented than the prototype data, because it was only run for 50 simulations per scenario. This also kept the computing time sane. The prototype was run at 5,000 simulations per scenario, and this takes considerably longer to compute. This chart above provides an interesting contrast for the final product.

The advantage of printing these files out as .csv files was this allowed the raw comma separated value-data to be actually literally copied and pasted into one line of R code in the app.R file used for Hooply. This eliminated any need to zip other files for the app, allowing it to be hosted easily on the [ShinyApps.io server via this link - please take a moment to check out the app yourself](https://carlrlarson.shinyapps.io/hooply_app/).

Just printing out a contour map is good but insufficient. The data that goes into the contour map was a computational bottleneck, and took about 20 or 25 minutes per run, to compute (in other words - not something to attempt on the user's machine). The above function was run with a "MinzLeft" of 3.1, a "scdiff" or score difference of 9, and then looped over an "m" value of 9,000 times. This signifies 9,000 basketball game-flow tables generated and analyzed for each of the 13 by 23 map (13 * 23 = 299) of relevant end-game score-time combinations. This was a double for-loop where the y-loop was nested inside the x-loop for the time-variable, or 23 different score scenarios for each semi-randomly selected "moment," or average time. Those specific values of score difference and time only frame the eventual graph, and aren't meant to refer to a specific user. 

##Product: Hooply- Basketball's 'Garbage-Time' App

The final step was to convert this presentation of data into an actionable recommender application that was easy to use. 

Below is the current prototype for the Hooply web application, which can easily be run and hosted out of this R Markdown document as a viable Shiny Application. 

The data for the app was taken from the raw .csv outputs of the above "gftopo4" function, and pasted into the "matrix()" function below as the "data" argument, which conveniently uses comma separated data. This use of copy and paste was much easier than zipping files and having multiple files as the application. 

Additionally, the x-axis was reversed to create this app after some brief user testing revealed it to be more intuitive to keep time flowing left-to-right as shown in a typical game-flow chart. 

```{r}

#
# This is a Shiny web application. You can run the application by clicking
# the 'Run App' button above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
#

library(shiny)

# Define UI 
ui <- fluidPage(   
  # App title ----
  titlePanel("Hooply"),
  
  headerPanel("The Basketball 'Garbage Time' App"),
  
  # Sidebar area 
  sidebarPanel(
    
    h2("Is It Garbage Time Or Not?"),
    
    h4("Please Select Your Scenario:"),
    
    numericInput(inputId = "ScoreA", 
                 label = "Score of Team A / Away Team", 
                 value=66, min = 1, max = 250, step = 1),
    
    numericInput(inputId = "ScoreB", 
                 label = "Score of Team B / Home Team", 
                 value=83, min = 1, max = 250, step = 1),
    
    numericInput(inputId = "WholeMinutesLeft", 
                 label = "Minutes Remaining:",
                 min = 0, max = 12, value = 10, step = 1),
    
    numericInput(inputId = "SecondsLeft", 
                 label = "Seconds Remaining:",
                 min = 0, max = 59, value = 12, step = 1),
    
    numericInput(inputId = "ConfTh",
                 label = "Confidence Threshold:",
                 min = 0.8, max = 1, value = 0.995, step = 0.005)
  ),
  
  mainPanel(
    # Output: topographical style contour map of probability field
    
    plotOutput("contourP"),
    textOutput("rec"),
    tags$head(tags$style("#rec{color: black;
                         font-size: 18px;
                        font-style: italic;
                         }"
                      )
    ),
    textOutput("num"),
    tags$head(tags$style("#num{color: black;
                         font-size: 36px;
                         font-style: bold;
                         }"
                      )
              ),
    verbatimTextOutput("blurb")
  ))

# Define server logic;
server <- function(input, output) {
  
  output$contourP <- renderPlot({
    dfz <<- matrix(data = c(0.851218063,0.8644320298,0.8789817232,0.8888583597,0.9000288517,0.9110491244,0.9185416325,0.927913844,0.94008038,0.9488792764,0.9581502406,0.9557178726,0.9724770642,0.9754395036,0.9787946429,0.9816596512,0.9866577718,0.9901129944,0.9867755737,0.9922746781,0.9947743468,0.9937597504,0.9988681381,
                           0.8549845349,0.8769441334,0.8889907056,0.8979790941,0.9085205719,0.9174466779,0.9325934389,0.9378247316,0.9437545654,0.9545990566,0.9607508532,0.9645608629,0.974892396,0.975748194,0.9781573789,0.9855942377,0.985035784,0.9897345133,0.9937568858,0.9951923077,0.9946308725,0.9969834087,0.9956043956,
                           0.863324663,0.8815165877,0.8877019281,0.898495212,0.9206510682,0.9270249811,0.9304571797,0.9508307901,0.9509750319,0.9604880883,0.9642122583,0.9701195219,0.9693973635,0.9816679577,0.9879711307,0.988422576,0.9900545396,0.993576741,0.9940097342,0.99475595,0.9960801394,0.995728524,0.9973958333,
                           0.8798573975,0.8904856293,0.8987308987,0.9084294344,0.9201038362,0.9327083018,0.9384069655,0.9467185386,0.9526796917,0.9686184462,0.9701098007,0.9758930528,0.9829359514,0.9854500617,0.988590057,0.9927409988,0.9940494832,0.9926224011,0.9948148148,0.9959790913,0.9964633068,0.9972665148,0.9979317477,
                           0.882095035,0.8960335058,0.8982985305,0.9191115986,0.9324791247,0.94375094,0.9493570408,0.9535864979,0.9658802178,0.9717141241,0.9752692461,0.982452386,0.9850306192,0.9865805169,0.9923300714,0.9907887162,0.9940316323,0.9948018194,0.9971860711,0.9961759082,0.9978577549,0.9990770651,0.9990128332,
                           0.8863232682,0.9110864472,0.9191385092,0.9297107326,0.9357811171,0.9498940357,0.9563549161,0.9604059225,0.9728085868,0.9758034725,0.9814,0.9830721003,0.9882592007,0.9891196835,0.9919689119,0.9949565705,0.9958456973,0.9983665469,0.9976327359,0.998492841,0.9995972614,0.9982158787,1,
                           0.8918983131,0.9094742063,0.9252191851,0.9359989083,0.9487760011,0.9557217651,0.9634609301,0.9662276258,0.9772009821,0.9810856819,0.9857000993,0.9867916489,0.9905469278,0.9942238267,0.9935450555,0.9968900198,0.9988399072,0.9981197117,0.998648192,0.9985261606,0.9995972614,1,1,
                           0.9075798656,0.9211403184,0.9318503279,0.9448584203,0.954738331,0.9593447506,0.9729560732,0.976778902,0.9832643202,0.9847901778,0.9886051081,0.9924098672,0.9955446647,0.9974256962,0.997755611,0.9975536831,1,0.9984553599,1,0.9992912828,0.9996149403,1,1,
                           0.92294076,0.9275326596,0.940227214,0.9479209271,0.9631095406,0.9679200239,0.9748447205,0.9839563348,0.9877750611,0.9891444342,0.9926313748,0.9968808484,0.9964850615,0.9971916686,0.9984658655,0.9994742376,0.9994174192,1,1,1,1,1,1,
                           0.9249882242,0.9412630288,0.9547738693,0.9605068068,0.9732079236,0.9807093041,0.9821018816,0.9875552645,0.9924190214,0.9946236559,0.9960098803,0.9975344155,0.9989032683,0.9990435198,0.9987639061,0.9989293362,0.9994444444,1,1,1,1,1,1,
                           0.9415391913,0.9537671233,0.9601282051,0.9743931715,0.9808045166,0.986129362,0.9899115319,0.9936264095,0.9961937716,0.9965385316,0.9982922201,0.9989829129,0.9993581515,1,0.9997589781,1,1,1,1,1,1,1,1,
                           0.9544058587,0.9642989043,0.9744567289,0.9810534802,0.988943317,0.9929370218,0.995537775,0.9973886078,0.9984680851,0.9998200792,1,1,1,1,1,1,1,1,1,1,1,1,1,
                           0.9694971146,0.9799359921,0.9861467419,0.9926362297,0.9977891391,0.9997023367,0.9998476539,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
                           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),
                  nrow = 14, ncol = 23, byrow = TRUE)

    xax <- as.matrix(c(-13.01613896, -12.08262521, -11.14517178, -10.22593366, -9.302148293, -8.366343087, -7.448042951, -6.501414602, -5.576598384, -4.652290142, -3.723155889, -2.787807794,  -1.861513943, 0))
    yax <<- as.matrix(seq(from = 4, to = 26))
    scdiff <- abs(as.numeric(input$ScoreA) - as.numeric(input$ScoreB))
    MinzLeft <- -1*(as.numeric(input$WholeMinutesLeft) + (as.numeric(input$SecondsLeft) / 60))
     if(MinzLeft < 0){
    contour(x = xax, 
            y = yax, 
            z = dfz,
            levels = c(seq(from = 0.9, to = 0.975, by = 0.025), 0.998),
            labels = c("90%", "92.5%", "95%", "97.5%", "100%"),
            col = topo.colors(5),
            xlab = "Minutes Remaining", ylab = "Point Lead",
            labcex = 2)
            #For some reason including a legend breaks the app
            #legend(x=2,y=26,legend=c("90%", "92.5%", "95%", "97.5%", "100%"), fill = topo.colors(5), col = topo.colors(5)),
            abline(h = scdiff, v = MinzLeft, col = "blue", lty=3, lwd=6)
            title(main = "Percent Win Chance For Leading Team")
    }
  })
  
  output$rec <- renderText({
    scdiff <- abs(as.numeric(input$ScoreA) - as.numeric(input$ScoreB))
    MinzLeft <- as.numeric(input$WholeMinutesLeft) + (as.numeric(input$SecondsLeft) / 60)
    x_index <<- round(MinzLeft * 0.4847 + 0.4733)
    y_index <<- as.numeric(scdiff - 4)
    if(MinzLeft == 0){
      print("Game over")} 
    else if(MinzLeft != 0 && scdiff < 4.5){
      print("The score is too close to make a prediction")
    }
    else if(scdiff > 25){
      print("A score this wide indicates a big blowout.")
    }
    else if(input$ConfTh > (dfz[x_index, y_index])){
      print("No, it is not garbage time - the leaders' % win chance is less than:")
    } else if(scdiff>5) {
      print("Yes, it is garbage time - the leaders' % win chance is roughly:")
    }
})
  
  output$num <- renderText({
    scdiff <- abs(as.numeric(input$ScoreA) - as.numeric(input$ScoreB))
    MinzLeft <- as.numeric(input$WholeMinutesLeft) + (as.numeric(input$SecondsLeft) / 60)
    x_index <<- round(MinzLeft * 0.4847 + 0.4733)
    y_index <<- as.numeric(scdiff - 4)
    val <- (dfz[x_index, y_index]) * 100
    if(MinzLeft == 0){
      print("Game over")} 
    if(scdiff < 5.5 | scdiff > 25){
      print("Please enter a score difference between 6 and 25.")
    } else if(MinzLeft != 0) {
      print(floor(val))
      }
  })
  
  output$blurb <- renderText({
    print("The blue lines intersect at the location of your scenario on the map.

For preseason-type urgency, coaches use a confidence threshold of 85%. 

For playoffs-type urgency, coaches use a confidence threshold of 99.9%.

Default is set to a 99.5% figure that should work fine for most users.

Created using 2.4 million simulated basketball games.

Hooply quantifies the end-game of basketball.
          
Copyright 2018")
  })
  
  }


# Run the app
shinyApp(ui = ui, server = server)


```

A confidence interval variable is included, so coaches can edit their "risk," meaning they can put Confidence Interval closer to 90% to indicate a conservative approach towards star player fatigue or injury, showing more indifference to losing a game, perhaps like in the pre-season when games don't count. 

A coach-user selecting a much higher confidence interval of perhaps 100% would be playing his star players as aggressively as possible, perhaps in a situation like the NBA finals where games matter the most. 

Ultimately, it's not a necessary detail to edit, so the app's confidence threshold has been defaulted to 99.5%, which is plenty high, and still shows enough room on the map to help users make a meaningful decision. 

Given the roughly 14% bias compared to Goel's data, it's worth noting that the default confidence interval for the ultimate recommender is still set at 99.5%, meaning this linear bias can be offset by up-adjusting our linear confidence threshold.

##App Details

The y-range, score difference between team A and team B, is kept between 4 and 25. This is because with a score less than 4, the game is truly a close game, and there's no way to realistically predict garbage time. Scores that close just aren't relevant. Similarly, scores differences beyond 25 aren't relevant either, since everyone knows that is a blowout and the leading team is really going to win. If there's a 30 point lead, no one is going to be surprised to punch in the app and confirm that the leading team is actually going to win, that just isn't impressive at all. 

Similarly, there is a minimum value for time remaining at 5 seconds. Time-values less than 4 seconds were actually breaking the app for some unknown reason, and the easiest fix was to cap the minimum time value at 5 seconds; if there's no time left in the game, there's no point in trying to make a "prediction."

To create Hooply, this code was run for "gftopo4(m=9000)" which yields slightly over 2.6 million games when multiplied out in the double for-loop over the 13 by 23 grid of the x and y axis. To run this many simulations, it takes roughly 20-30 minutes; so I have changed the code above to just show 90 simulations, which wouldn't take as long. Feel free to challenge your computer and take m up to 10,000 or more, and see how "in-focus" this grid can become.

Next, these above 3 matrices (dfz or the z-matrix, xax or the x axis labels, and yax or the y axis label) needed to be exported into the ultimate app.R file for the web app. This proved difficult because having any kind of file-path or file tree in your app is extremely tricky and was personally over my head for this project. It was initially my approach to try to link to files on my own machine in the app directory, but I still couldn't get it to work that way. Ultimately, I just had to hard-code the data into the app.R file itself. So rather than trying to unzip and link to other files, I just opened each .csv in notepad, and copied the comma-separated data, and pasted it into the stated data for the z-matrix. 

Initially my thought was to graph out many game-flow charts on the same graph and then run analyses on that big set of graphed data. Though this is indeed possible, the resulting graphs ended up too cluttered, and I couldn't gain any insight. Overall, these types of graphs contain too much information, in a sense. 

Ultimately, we are only concerned with the end-game, and the user's position in that end-game, as well as the eventual game outcome. With respect to the end-game analysis, what happened on the scoreboard to get you up to that point really has nothing to do with how the game will end.

Graphically depicting all the actual action itself for a large number of whole games is perhaps superficially interesting, but the real key is in reducing the dimension of that information, and focusing on the user's own end-game scenario.

#Contour Map Matrix

Even though Goel's write up on basketball end-games was helpful and provided real-world empirical data, the 

Contour maps are essentially a type of 3-dimensional graph, where the third dimension "z" is compressed into a line and made into "altitude," which creates the contours. This is ideal for our purposes as R's contour function draws piece-wise regression lines for whatever contour bands we want. For visual design purposes, this was chosen to be 5, so as not to overwhelm the viewer but still provide robust information.

So essentially this needed to be a matrix along with two vectors representing non-index values for the x and y axis, minutes left, and score difference, respectively. Together, these 3 matrices (two of which only have one column) go into the contour function and create the chart. 

This block of code is what was run to create the matrices used in the final app. Note this code both produces the 3 needed matrices in .csv format, as well as provides an early snapshot glimpse of the contour map itself.

Initially my thought was to graph out many game-flow charts on the same graph and then run analyses on that big set of graphed data, and though this is indeed possible, the resulting graphs ended up too cluttered, and I couldn't gain any insight. Overall, these types of graphs contain too much information, in a sense. Ultimately, we are only concerned with the user's position, and with the ultimate result of the game. Graphically depicting the actual action itself for a large number of games is perhaps superficially interesting but the real key is in gleaning much smaller amounts of information, and keeping that information of the utmost relevance. 

##Ideas For Further Research

It's easy to envision trying to plot different "pet" algorithms on this chart, like Goel's 2*sqrt(Minutes-remaining), or some similar relationship. This makes an excellent initial topic for further exploration beyond this introductory data science course thesis project, the process of measuring the success of "pet" algorithms - Hooply works around this type of analysis by displaying multiple layers of potential "pet" algorithms next to each other (simply slicing at regular confidence intervals). 

One future potential product to break out would be an expanded range for time values. Right now, Hooply is only for the 4th quarter. But what about the 3rd quarter? It would require more computations, and simulations, and over all, that would be one example of an appropriate product to release. 

Another one would be to expand the data where there is less than one minute left, and slice (or index) the final minute a dozen times or so. 

Running many simulations on the back-end machine limits us one way on computation load, and on the other side we are limited by the time-zoom of the x-axis to display for the user. 

In some brief user testing, it's become apparent that some users are asking the thesis problem question with 15 seconds left, and want to know the nuance of that probability map, while other users are positing the thesis problem statement with 18 minutes left (in the 3rd quarter); and both of these are valid, but so far, Hooply has published somewhat of a compromise on the "view" setting, essentially trying to encapsulate the greatest number of users' potential preference, but due to the limits of computing power, and the graphical challenges of zooming out from one minute to 20 or more, Hooply has quite a difficult and interesting challenge here in displaying this data in a way that maximizes the number of satisfied users. 

It's plausible that a logarithmic scale would better suit the display, and a logarithmic x-axis could even be another Hooply spin-off product idea - it's definitely the case that this illuminates a lot of different product ideas.

It would also be interesting to see this type of analysis applied to other timed sports, such as football, or hockey. 

Data mining for this topic was a particular headache, and while there is a treasure trove archive of game-flow charts at [PopcornMachine.net](http://popcornmachine.net), it requires a click and hover with a mouse to view the raw data, so it's good for casual verification, but more than ever, I personally see the need to expand my programming skill-set to encompass advanced data scraping and mining.

Now that it has been defined more quantitatively, there are a number of different pressing questions at hand regarding "Garbage Time," and the effect it might have on a player to be taken out consistently during garbage time, or not. 

I have my hunches Gregg Popovich, head coach of the San Antonio Spurs, for one, has his own mental version of this analysis, because he takes his star players out of games earlier than other coaches, I have noticed; Popovich is also the longest-tenured coach in the NBA, with 5 championships, his players have some of the longest careers in sports - could Popovich's respect for garbage time be playing a part in extending his players' careers? This is a much more advanced question and well suited for a future project, but no doubt it is a scientific question that could be answered with more skilled data mining from PopcornMachine.net and the style of analysis outlined in this project. 

##Conclusion 

Basketball is a great sport, and hopefully this project is helpful for people thinking about the closing minutes of games in a serious and statistically rigorous way. 

Ultimately, the nuance of Hooply is in its scope - we are just here to rigorously quantify the end-game probabilities, and provide a framework for how other programmers can easily refine their own thinking into workable models.

##Recommendations

If the game is in garbage time, the recommendation is simple: *sit your star players!* 

Don't let heartbreaking injuries happen in garbage time, when there just isn't enough time to stage a plausible comeback. Furthermore, why needlessly fatigue your star players if the game is mathematically over?

##Glossary: 

#Garbage Time

Originally coined by Los Angeles Lakers' famous announcer Chick Hearn, "Garbage Time" refers to the end of a basketball game when one team is in the lead by a comfortable enough scoring margin that it's customary for the coach to put in the substitutes or non-starters, in place of the starters, so that the starters can rest and avoid injury and fatigue.

#"Overtime"

At the end of regulation time in basketball, 48 minutes, if the score is tied, there must be an overtime round of 5 minutes. 

#"Basketball-action"

A basketball-action is an official action in basketball recordable as one of the official statistics in basketball. These can include baskets, assists, rebounds, turnovers, fouls, blocked shots, or a number of other recordable statistics that may or may not affect the score. In many game-flow tables and charts, each basketball-action is given one point along the x-axis which represent time. For this analysis, the computational task was streamlined by initially deleting non-scoring basketball actions from the game-flow tables analyzed. Ultimately, this simply meant fewer points along horizontal stretches of the line representing score through time, so it's not much of a change aesthetically. 

#"N.B.A. or NBA"

This stands for the "National Basketball Association," and is the leading professional basketball league in the United States and the world. As of 2018, every NBA team was worth over $1 billion according to Forbes Magazine.

#"Starter"

A starter in any sport, including basketball, means a player who is considered "good" relative to their teammates. "Starters" are in the game when the game starts (at tip-off), and, typically when it finishes. Basketball is like hockey and football (and unlike baseball and soccer) in that players can re-enter the game after being substituted out by the coach. 

#"Bench player"

As opposed to a starter, a bench player just sits on the team bench for the start of the game, and swaps in during action at the coach's discretion. Managing which bench players and starters are in and out of the game at what times is one of the toughest dynamic challenges coaches face. 

#Tip-off

Tip-off is the beginning of a basketball game, or an overtime period. Like the game-starting face-off in hockey, or kick-off in football, basketball's tip off starts the game, and some other scenarios can lead to a tip-off during the game. In basketball, the team winning the game-opening tip off must give possession to the other team to start the second quarter and third quarter, but keeps possession to start the fourth. 

##Bibliography: 

1. Mertens, J.F. & Zamir, S. Int J Game Theory (1976) 5: 187. https://doi.org/10.1007/BF01761601

2. Westfall, Peter H. “Graphical Presentation of a Basketball Game.” The American Statistician, vol. 44, no. 4, 1990, pp. 305–307. JSTOR, JSTOR, www.jstor.org/stable/2684357.

3. Carlson, Michael. (2002, August 7) Chick Hearn. Retrieved from: https://www.theguardian.com/news/2002/aug/08/guardianobituaries1

4. Lewis, Michael. Moneyball. New York : W.W. Norton, 2004.

5. Goel, Sharad. "How Close Is Close?" Messy Matters, May 31, 2012. http://messymatters.com/moneyball/

6. Everson, Patrick. "Halftime Lines: How Books Set The Odds And How Sharps Bet Them." Covers, January 12, 2015. https://www.covers.com/editorial/Article/31e4a81b-b51e-e711-80cb-44a8423171c1/Halftime-lines-How-books-set-them-and-how-sharps-bet-them.

7. Haberstroh, Tom. (2015, January 28). Home-court advantage? Not so much. Retrieved from: http://www.espn.com/nba/story/_/id/12241619/home-court-advantage-decline.

8. Kenter, Franklin. “An Analysis of the Basketball Endgame : When to Foul When Trailing and Leading !” (2015).

9. Groves, Roger. (2015, December 12). Why The NBA Will Eventually Overtake The NFL In Popularity And Why It Matters. Retrieved from: https://www.forbes.com/sites/rogergroves/2015/12/12/why-the-nba-will-eventually-overtake-the-nfl-in-popularity-and-why-it-matters/#4448dfd1e4c6.

10. Badenhausen, Kurt. (2016, December 15) The Average Player Salary And Highest-Paid In NBA, MLB, NHL, NFL And MLS. Retrieved from: https://www.forbes.com/sites/kurtbadenhausen/2016/12/15/average-player-salaries-in-major-american-sports-leagues/#1056de0e1050.

11. Minter, Adam. (2017, September 28). China Is Hoops Country. Retrieved from: https://www.bloomberg.com/view/articles/2017-09-28/basketball-not-soccer-is-china-s-game-of-choice.

12. Friedell, Nick. (2017, October 24) The Story Of The Chicago Bulls’ Downfall. Retrieved from: http://www.espn.com/nba/story/_/id/21082183/story-chicago-bulls-downfall.

13. Bonesteel, Matt. (2018, January 25) "If sports gambling is legalized, the NBA wants in on the profits." Retrieved from: https://www.washingtonpost.com/news/early-lead/wp/2018/01/25/if-sports-gambling-is-legalized-the-nba-wants-in-on-the-profits/?noredirect=on&utm_term=.f3b1c064eb0a

